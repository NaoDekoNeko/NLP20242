{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "37e5cdff",
      "metadata": {
        "id": "37e5cdff"
      },
      "source": [
        "### Prueba de entrada\n",
        "\n",
        "Un pipeline de NLP es una secuencia estructurada de pasos que permite a una computadora procesar y analizar texto de manera sistemática, extrayendo información significativa. Cada paso en el pipeline se enfoca en una tarea específica del procesamiento del lenguaje, y juntos forman un sistema completo para entender y manipular el lenguaje natural.\n",
        "\n",
        "Los pasos fundamentales en un pipeline de NLP incluyen:\n",
        "\n",
        "1. **Segmentación de oraciones:** Dividir el texto en oraciones individuales, lo cual facilita el procesamiento posterior al tratar cada oración como una unidad de análisis.\n",
        "2. **Tokenización:** Separar las oraciones en palabras o tokens, lo que permite el análisis detallado de cada unidad de texto.\n",
        "3. **Predicción de partes gramaticales (POS):** Identificar la categoría gramatical de cada token (como sustantivo, verbo, adjetivo), lo cual ayuda a comprender la estructura gramatical de las oraciones.\n",
        "4. **Lematización:** Reducir las palabras a su forma básica o lema, para asegurar que diferentes formas de una palabra se traten como una misma entidad.\n",
        "5. **Identificación de stopwords:** Marcar palabras comunes y de poca relevancia (como \"and\", \"the\"), que pueden ser filtradas para reducir el ruido en el análisis.\n",
        "6. **Análisis de dependencias:** Entender las relaciones gramaticales entre las palabras de una oración, como identificar qué palabras dependen de otras.\n",
        "7. **Reconocimiento de entidades nombradas (NER):** Detectar y etiquetar nombres de personas, lugares y organizaciones, para extraer información específica del texto.\n",
        "8. **Resolución de coreferencia:** Determinar a qué entidad se refieren los pronombres en el texto, para entender mejor la referencia de entidades en el contexto.\n",
        "\n",
        "Cada uno de estos pasos contribuye a la extracción de información significativa de un texto, permitiendo a una computadora \"entender\" el contenido de manera más efectiva. Este proceso es fundamental en muchas aplicaciones de NLP, como la traducción automática, la generación de texto, y el análisis de sentimientos.\n",
        "\n",
        "**Nota:** Este proyecto deberá ser subido a tu repositorio personal a más tardar el 7 de septiembre.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ba7450",
      "metadata": {
        "id": "c0ba7450"
      },
      "source": [
        "**Pregunta**\n",
        "\n",
        "Desarrolla un pipeline simplificado de NLP que incluya tres componentes clave: tokenización usando expresiones regulares (Regex), codificación Byte Pair Encoding (BPE) para la compresión de tokens, y cálculo de distancia de edición para comparar la similitud entre tokens.\n",
        "\n",
        "**Instrucciones:**\n",
        "\n",
        "1. **Tokenización con regex:**\n",
        "   - Implementa un tokenizador utilizando expresiones regulares. El tokenizador debe ser capaz de separar el texto en palabras, respetando la puntuación como tokens independientes.\n",
        "   - Por ejemplo, usando el texto: `\"London is the capital and most populous city of England and the United Kingdom.\"`, el resultado esperado sería:\n",
        "     ```\n",
        "     [\"London\", \"is\", \"the\", \"capital\", \"and\", \"most\", \"populous\", \"city\", \"of\", \"England\", \"and\", \"the\", \"United\", \"Kingdom\", \".\"]\n",
        "     ```\n",
        "\n",
        "2. **Aplicación de byte pair encoding (BPE):**\n",
        "   - Implementa el algoritmo de Byte Pair Encoding para comprimir la representación de los tokens.\n",
        "   - Aplica BPE sobre el conjunto de tokens obtenido en el paso anterior y muestra cómo el vocabulario se reduce mediante la combinación de pares de caracteres más frecuentes.\n",
        "   - Evalúa cómo BPE afecta el tamaño del vocabulario y la representación de palabras raras.\n",
        "\n",
        "3. **Cálculo de distancia de edición:**\n",
        "   - Implementa un algoritmo de distancia de edición (por ejemplo, la distancia de Levenshtein) para comparar la similitud entre diferentes tokens en el texto.\n",
        "   - Utiliza la distancia de edición para identificar palabras similares dentro del texto tokenizado. Por ejemplo, compara palabras como `\"London\"` y `\"Londinium\"` y analiza su similitud.\n",
        "\n",
        "4. **Integración en un pipeline:**\n",
        "   - Integra los tres componentes anteriores en un pipeline simple que procese un texto de entrada desde la tokenización hasta la evaluación de similitudes.\n",
        "   - Evalúa el desempeño del pipeline en un texto de prueba, y discute posibles mejoras o ajustes.\n",
        "\n",
        "**Criterios de evaluación:**\n",
        "\n",
        "- Correctitud en la implementación de cada componente del pipeline.\n",
        "- Eficiencia en la aplicación del algoritmo BPE y en la reducción del vocabulario.\n",
        "- Precisión en el cálculo de la distancia de edición y en la identificación de palabras similares.\n",
        "- Calidad del código (documentación, modularidad, legibilidad) y análisis crítico de los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f88b0986",
      "metadata": {
        "id": "f88b0986"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "# Dividir oraciones en palabras\n",
        "def sentence_segmentation(sentence):\n",
        "    pattern = re.compile(r'[¿¡]?\\b\\w*[.!?]?\\b')\n",
        "    return [word for word in pattern.findall(sentence) if word.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "BdaNBrYPl7PY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdaNBrYPl7PY",
        "outputId": "79364026-fd86-405d-ff9e-b85e4daf98fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulario inicial: {'l o w </w>': 1, 'l o w e s t </w>': 1, 'l o w e r </w>': 1, 'l o w e r i n g </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 1:\n",
            "Par más frecuente: ('l', 'o')\n",
            "Vocabulario actualizado: {'lo w </w>': 1, 'lo w e s t </w>': 1, 'lo w e r </w>': 1, 'lo w e r i n g </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 2:\n",
            "Par más frecuente: ('lo', 'w')\n",
            "Vocabulario actualizado: {'low </w>': 1, 'low e s t </w>': 1, 'low e r </w>': 1, 'low e r i n g </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 3:\n",
            "Par más frecuente: ('low', 'e')\n",
            "Vocabulario actualizado: {'low </w>': 1, 'lowe s t </w>': 1, 'lowe r </w>': 1, 'lowe r i n g </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 4:\n",
            "Par más frecuente: ('s', 't')\n",
            "Vocabulario actualizado: {'low </w>': 1, 'lowe st </w>': 1, 'lowe r </w>': 1, 'lowe r i n g </w>': 1, 'n e w e st </w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 5:\n",
            "Par más frecuente: ('st', '</w>')\n",
            "Vocabulario actualizado: {'low </w>': 1, 'lowe st</w>': 1, 'lowe r </w>': 1, 'lowe r i n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 6:\n",
            "Par más frecuente: ('lowe', 'r')\n",
            "Vocabulario actualizado: {'low </w>': 1, 'lowe st</w>': 1, 'lower </w>': 1, 'lower i n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 7:\n",
            "Par más frecuente: ('low', '</w>')\n",
            "Vocabulario actualizado: {'low</w>': 1, 'lowe st</w>': 1, 'lower </w>': 1, 'lower i n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 8:\n",
            "Par más frecuente: ('lowe', 'st</w>')\n",
            "Vocabulario actualizado: {'low</w>': 1, 'lowest</w>': 1, 'lower </w>': 1, 'lower i n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 9:\n",
            "Par más frecuente: ('lower', '</w>')\n",
            "Vocabulario actualizado: {'low</w>': 1, 'lowest</w>': 1, 'lower</w>': 1, 'lower i n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Paso 10:\n",
            "Par más frecuente: ('lower', 'i')\n",
            "Vocabulario actualizado: {'low</w>': 1, 'lowest</w>': 1, 'lower</w>': 1, 'loweri n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Vocabulario final: {'low</w>': 1, 'lowest</w>': 1, 'lower</w>': 1, 'loweri n g </w>': 1, 'n e w e st</w>': 1, 'w i d e r </w>': 1}\n",
            "\n",
            "Palabra original: ['l', 'o', 'w', 'e', 's', 't', '</w>']\n",
            "\n",
            "Tokenización de 'lowest': ['lowest</w>']\n",
            "\n",
            "Palabra original: ['l', 'o', 'w', 'e', 'r', 'i', 'n', 'g', '</w>']\n",
            "\n",
            "Tokenización de 'lowering': ['loweri', 'n', 'g']\n",
            "\n",
            "Palabra original: ['n', 'e', 'w', 'e', 's', 't', '</w>']\n",
            "\n",
            "Tokenización de 'newest': ['n', 'e', 'w', 'e', 'st</w>']\n"
          ]
        }
      ],
      "source": [
        "# Intento de mejorar/corregir el algoritmo visto en clase\n",
        "\n",
        "class BytePairEncoding:\n",
        "    def __init__(self, num_merges):\n",
        "        self.num_merges = num_merges\n",
        "        self.vocab = {}\n",
        "        self.merges = []\n",
        "\n",
        "    def train(self, corpus):\n",
        "        # Inicializamos el vocabulario con las palabras separadas por caracteres y añadimos el token de fin de palabra\n",
        "        self.vocab = {}\n",
        "        tokenized_corpus = [' '.join(list(word)) + ' </w>' for word in corpus]  # Añadimos el token de fin de palabra '</w>'\n",
        "\n",
        "        # Contamos la frecuencia de cada palabra en el corpus tokenizado\n",
        "        for word in tokenized_corpus:\n",
        "            if word in self.vocab:\n",
        "                self.vocab[word] += 1\n",
        "            else:\n",
        "                self.vocab[word] = 1\n",
        "\n",
        "        print(f\"Vocabulario inicial: {self.vocab}\")\n",
        "\n",
        "        # Realizamos los merges del BPE\n",
        "        for merge_step in range(self.num_merges):\n",
        "            pairs = self.get_pair_frequencies()\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            # Encontramos el par más frecuente\n",
        "            most_frequent_pair = max(pairs, key=pairs.get)\n",
        "            self.merges.append(most_frequent_pair)\n",
        "\n",
        "            # Actualizamos el vocabulario uniendo los pares más frecuentes\n",
        "            self.vocab = self.merge_vocabulary(most_frequent_pair)\n",
        "\n",
        "            print(f\"\\nPaso {merge_step + 1}:\")\n",
        "            print(f\"Par más frecuente: {most_frequent_pair}\")\n",
        "            print(f\"Vocabulario actualizado: {self.vocab}\")\n",
        "\n",
        "    def get_pair_frequencies(self):\n",
        "        \"\"\"Obtiene las frecuencias de cada par de símbolos en el vocabulario.\"\"\"\n",
        "        pairs = {}\n",
        "        for word, freq in self.vocab.items():\n",
        "            tokens = word.split()\n",
        "            for i in range(len(tokens) - 1):\n",
        "                pair = (tokens[i], tokens[i + 1])\n",
        "                if pair in pairs:\n",
        "                    pairs[pair] += freq\n",
        "                else:\n",
        "                    pairs[pair] = freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_vocabulary(self, pair):\n",
        "        \"\"\"Une los pares de símbolos más frecuentes en el vocabulario.\"\"\"\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(pair)  # Formamos el bigrama como una cadena\n",
        "\n",
        "        # Reemplazamos los pares más frecuentes con el nuevo token\n",
        "        for word in self.vocab:\n",
        "            new_word = word.replace(bigram, ''.join(pair))  # Reemplazamos el par con el token unido\n",
        "            new_vocab[new_word] = self.vocab[word]\n",
        "\n",
        "        return new_vocab\n",
        "\n",
        "    def tokenize(self, word):\n",
        "        \"\"\"Tokeniza una palabra basándose en las fusiones aprendidas por BPE.\"\"\"\n",
        "        tokens = list(word) + ['</w>']  # Añadimos el token de fin de palabra '</w>'\n",
        "        print(f\"\\nPalabra original: {tokens}\")\n",
        "\n",
        "        # Convertimos la palabra en su representación BPE paso a paso\n",
        "        while True:\n",
        "            # Convertimos a cadena con espacios para facilitar la búsqueda de los merges\n",
        "            current_word = ' '.join(tokens)\n",
        "            applied = False\n",
        "\n",
        "            # Intentamos aplicar cada regla de merge en el orden en que fue aprendida\n",
        "            for merge in self.merges:\n",
        "                bigram = ' '.join(merge)\n",
        "                if bigram in current_word:\n",
        "                    # Aplicamos la unión reemplazando el bigrama por el nuevo token\n",
        "                    current_word = current_word.replace(bigram, ''.join(merge))\n",
        "                    tokens = current_word.split()\n",
        "                    applied = True\n",
        "                    break\n",
        "\n",
        "            # Si no se puede aplicar más merges, salimos del bucle\n",
        "            if not applied:\n",
        "                break\n",
        "\n",
        "        # Removemos el token de fin de palabra '</w>' para la tokenización final\n",
        "        if tokens[-1] == '</w>':\n",
        "            tokens = tokens[:-1]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        \"\"\"Devuelve el vocabulario actual.\"\"\"\n",
        "        return self.vocab\n",
        "\n",
        "# Ejemplo de uso\n",
        "bpe = BytePairEncoding(num_merges=10)\n",
        "corpus = [\"low\", \"lowest\", \"lower\", \"lowering\", \"newest\", \"wider\"]\n",
        "bpe.train(corpus)\n",
        "\n",
        "print(\"\\nVocabulario final:\", bpe.get_vocabulary())\n",
        "\n",
        "# Ejemplos de tokenización\n",
        "print(\"\\nTokenización de 'lowest':\", bpe.tokenize('lowest'))\n",
        "print(\"\\nTokenización de 'lowering':\", bpe.tokenize('lowering'))\n",
        "print(\"\\nTokenización de 'newest':\", bpe.tokenize('newest'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "o--bjy-HnG7A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o--bjy-HnG7A",
        "outputId": "35968bc2-55b5-4706-dc6a-cc82528b367b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distancia de Levenshtein entre 'London' y 'Londinium' es 4\n"
          ]
        }
      ],
      "source": [
        "class LevenshteinDistance:\n",
        "    @staticmethod\n",
        "    def calculate(s1, s2):\n",
        "        \"\"\"Calcula la distancia de Levenshtein entre dos cadenas s1 y s2.\"\"\"\n",
        "        m = len(s1)\n",
        "        n = len(s2)\n",
        "\n",
        "        # Crear una matriz de (m+1) x (n+1) para almacenar distancias\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        # Inicializar la primera fila y la primera columna\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        # Llenar la matriz\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i - 1] == s2[j - 1]:\n",
        "                    cost = 0\n",
        "                else:\n",
        "                    cost = 1\n",
        "                dp[i][j] = min(dp[i - 1][j] + 1,      # Eliminación\n",
        "                               dp[i][j - 1] + 1,      # Inserción\n",
        "                               dp[i - 1][j - 1] + cost)  # Sustitución\n",
        "\n",
        "        # El valor en dp[m][n] es la distancia de Levenshtein\n",
        "        return dp[m][n]\n",
        "\n",
        "# Ejemplo de uso:\n",
        "levenshtein = LevenshteinDistance()\n",
        "s1 = \"London\"\n",
        "s2 = \"Londinium\"\n",
        "distancia = levenshtein.calculate(s1, s2)\n",
        "\n",
        "print(f\"Distancia de Levenshtein entre '{s1}' y '{s2}' es {distancia}\")\n",
        "\n",
        "# https://es.wikipedia.org/wiki/Distancia_de_Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2NoOz7Y3tID6",
      "metadata": {
        "id": "2NoOz7Y3tID6"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Pipeline principal\n",
        "def process_text(text, bpe, levenshtein):\n",
        "    print(f\"Texto de entrada: {text}\")\n",
        "\n",
        "    # Paso 1: Segmentación de palabras\n",
        "    words = sentence_segmentation(text)\n",
        "    print(f\"\\nSegmentación de palabras: {words}\")\n",
        "\n",
        "    # Paso 2: Tokenización BPE\n",
        "    tokenized_words = [bpe.tokenize(word) for word in words]\n",
        "    print(f\"\\nTokenización BPE de cada palabra:\")\n",
        "    for word, tokens in zip(words, tokenized_words):\n",
        "        print(f\"'{word}': {tokens}\")\n",
        "\n",
        "    # Paso 3: Evaluación de similitud\n",
        "    print(\"\\nDistancias de Levenshtein entre todas las combinaciones de palabras:\")\n",
        "    for word1, word2 in combinations(words, 2):\n",
        "        dist = levenshtein.calculate(word1, word2)\n",
        "        print(f\"Distancia entre '{word1}' y '{word2}': {dist}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "pgvKqN3It7zx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgvKqN3It7zx",
        "outputId": "474bbb0a-ef2f-4359-d079-9bf8ea1c9952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto de entrada: London is the capital and most populous city of England and the United Kingdom.\n",
            "\n",
            "Segmentación de palabras: ['London', 'is', 'the', 'capital', 'and', 'most', 'populous', 'city', 'of', 'England', 'and', 'the', 'United', 'Kingdom']\n",
            "\n",
            "Palabra original: ['L', 'o', 'n', 'd', 'o', 'n', '</w>']\n",
            "\n",
            "Palabra original: ['i', 's', '</w>']\n",
            "\n",
            "Palabra original: ['t', 'h', 'e', '</w>']\n",
            "\n",
            "Palabra original: ['c', 'a', 'p', 'i', 't', 'a', 'l', '</w>']\n",
            "\n",
            "Palabra original: ['a', 'n', 'd', '</w>']\n",
            "\n",
            "Palabra original: ['m', 'o', 's', 't', '</w>']\n",
            "\n",
            "Palabra original: ['p', 'o', 'p', 'u', 'l', 'o', 'u', 's', '</w>']\n",
            "\n",
            "Palabra original: ['c', 'i', 't', 'y', '</w>']\n",
            "\n",
            "Palabra original: ['o', 'f', '</w>']\n",
            "\n",
            "Palabra original: ['E', 'n', 'g', 'l', 'a', 'n', 'd', '</w>']\n",
            "\n",
            "Palabra original: ['a', 'n', 'd', '</w>']\n",
            "\n",
            "Palabra original: ['t', 'h', 'e', '</w>']\n",
            "\n",
            "Palabra original: ['U', 'n', 'i', 't', 'e', 'd', '</w>']\n",
            "\n",
            "Palabra original: ['K', 'i', 'n', 'g', 'd', 'o', 'm', '</w>']\n",
            "\n",
            "Tokenización BPE de cada palabra:\n",
            "'London': ['L', 'o', 'n', 'd', 'o', 'n']\n",
            "'is': ['i', 's']\n",
            "'the': ['t', 'h', 'e']\n",
            "'capital': ['c', 'a', 'p', 'i', 't', 'a', 'l']\n",
            "'and': ['a', 'n', 'd']\n",
            "'most': ['m', 'o', 's', 't']\n",
            "'populous': ['p', 'o', 'p', 'u', 'l', 'o', 'u', 's']\n",
            "'city': ['c', 'i', 't', 'y']\n",
            "'of': ['o', 'f']\n",
            "'England': ['E', 'n', 'g', 'l', 'a', 'n', 'd']\n",
            "'and': ['a', 'n', 'd']\n",
            "'the': ['t', 'h', 'e']\n",
            "'United': ['U', 'n', 'i', 't', 'e', 'd']\n",
            "'Kingdom': ['K', 'i', 'n', 'g', 'd', 'o', 'm']\n",
            "\n",
            "Distancias de Levenshtein entre todas las combinaciones de palabras:\n",
            "Distancia entre 'London' y 'is': 6\n",
            "Distancia entre 'London' y 'the': 6\n",
            "Distancia entre 'London' y 'capital': 7\n",
            "Distancia entre 'London' y 'and': 4\n",
            "Distancia entre 'London' y 'most': 5\n",
            "Distancia entre 'London' y 'populous': 6\n",
            "Distancia entre 'London' y 'city': 6\n",
            "Distancia entre 'London' y 'of': 5\n",
            "Distancia entre 'London' y 'England': 6\n",
            "Distancia entre 'London' y 'and': 4\n",
            "Distancia entre 'London' y 'the': 6\n",
            "Distancia entre 'London' y 'United': 6\n",
            "Distancia entre 'London' y 'Kingdom': 4\n",
            "Distancia entre 'is' y 'the': 3\n",
            "Distancia entre 'is' y 'capital': 6\n",
            "Distancia entre 'is' y 'and': 3\n",
            "Distancia entre 'is' y 'most': 3\n",
            "Distancia entre 'is' y 'populous': 7\n",
            "Distancia entre 'is' y 'city': 3\n",
            "Distancia entre 'is' y 'of': 2\n",
            "Distancia entre 'is' y 'England': 7\n",
            "Distancia entre 'is' y 'and': 3\n",
            "Distancia entre 'is' y 'the': 3\n",
            "Distancia entre 'is' y 'United': 5\n",
            "Distancia entre 'is' y 'Kingdom': 6\n",
            "Distancia entre 'the' y 'capital': 6\n",
            "Distancia entre 'the' y 'and': 3\n",
            "Distancia entre 'the' y 'most': 4\n",
            "Distancia entre 'the' y 'populous': 8\n",
            "Distancia entre 'the' y 'city': 4\n",
            "Distancia entre 'the' y 'of': 3\n",
            "Distancia entre 'the' y 'England': 7\n",
            "Distancia entre 'the' y 'and': 3\n",
            "Distancia entre 'the' y 'the': 0\n",
            "Distancia entre 'the' y 'United': 5\n",
            "Distancia entre 'the' y 'Kingdom': 7\n",
            "Distancia entre 'capital' y 'and': 6\n",
            "Distancia entre 'capital' y 'most': 6\n",
            "Distancia entre 'capital' y 'populous': 7\n",
            "Distancia entre 'capital' y 'city': 4\n",
            "Distancia entre 'capital' y 'of': 7\n",
            "Distancia entre 'capital' y 'England': 7\n",
            "Distancia entre 'capital' y 'and': 6\n",
            "Distancia entre 'capital' y 'the': 6\n",
            "Distancia entre 'capital' y 'United': 5\n",
            "Distancia entre 'capital' y 'Kingdom': 7\n",
            "Distancia entre 'and' y 'most': 4\n",
            "Distancia entre 'and' y 'populous': 8\n",
            "Distancia entre 'and' y 'city': 4\n",
            "Distancia entre 'and' y 'of': 3\n",
            "Distancia entre 'and' y 'England': 4\n",
            "Distancia entre 'and' y 'and': 0\n",
            "Distancia entre 'and' y 'the': 3\n",
            "Distancia entre 'and' y 'United': 4\n",
            "Distancia entre 'and' y 'Kingdom': 5\n",
            "Distancia entre 'most' y 'populous': 7\n",
            "Distancia entre 'most' y 'city': 4\n",
            "Distancia entre 'most' y 'of': 3\n",
            "Distancia entre 'most' y 'England': 7\n",
            "Distancia entre 'most' y 'and': 4\n",
            "Distancia entre 'most' y 'the': 4\n",
            "Distancia entre 'most' y 'United': 5\n",
            "Distancia entre 'most' y 'Kingdom': 7\n",
            "Distancia entre 'populous' y 'city': 8\n",
            "Distancia entre 'populous' y 'of': 7\n",
            "Distancia entre 'populous' y 'England': 7\n",
            "Distancia entre 'populous' y 'and': 8\n",
            "Distancia entre 'populous' y 'the': 8\n",
            "Distancia entre 'populous' y 'United': 8\n",
            "Distancia entre 'populous' y 'Kingdom': 7\n",
            "Distancia entre 'city' y 'of': 4\n",
            "Distancia entre 'city' y 'England': 7\n",
            "Distancia entre 'city' y 'and': 4\n",
            "Distancia entre 'city' y 'the': 4\n",
            "Distancia entre 'city' y 'United': 4\n",
            "Distancia entre 'city' y 'Kingdom': 6\n",
            "Distancia entre 'of' y 'England': 7\n",
            "Distancia entre 'of' y 'and': 3\n",
            "Distancia entre 'of' y 'the': 3\n",
            "Distancia entre 'of' y 'United': 6\n",
            "Distancia entre 'of' y 'Kingdom': 6\n",
            "Distancia entre 'England' y 'and': 4\n",
            "Distancia entre 'England' y 'the': 7\n",
            "Distancia entre 'England' y 'United': 5\n",
            "Distancia entre 'England' y 'Kingdom': 6\n",
            "Distancia entre 'and' y 'the': 3\n",
            "Distancia entre 'and' y 'United': 4\n",
            "Distancia entre 'and' y 'Kingdom': 5\n",
            "Distancia entre 'the' y 'United': 5\n",
            "Distancia entre 'the' y 'Kingdom': 7\n",
            "Distancia entre 'United' y 'Kingdom': 6\n"
          ]
        }
      ],
      "source": [
        "test_text = \"London is the capital and most populous city of England and the United Kingdom.\"\n",
        "bpe = BytePairEncoding(num_merges=10)\n",
        "levenshtein = LevenshteinDistance()\n",
        "process_text(test_text, bpe, levenshtein)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8bec96",
      "metadata": {},
      "source": [
        "En cuanto al análisis, si bien se intentó mejorar el algoritmo visto en clase no pude hacerlo, es un pequeño problema de lógica así que la mejora empieza por ahí.\n",
        "Por otro lado se ha establecido el pipeline de tal forma que acepta inyección de dependencias por si se quiere cambiar a otro tipo de algoritmo de tokenización como podría ser el caso del Scaffold-BPE, al igual que otro tipo de disntancia diferente a levenshtein.\n",
        "Sin embargo, aunque las mejoras están en el pipeline, el punto más flojo de este código está en mi implementación del algoritmo BPE, con un poco más de trabajo daría mejores resultados.\n",
        "Además, detalle importante es que si bien hay un pequeño tweak para que intente trabajar con oraciones en español al menos en la separación de oraciones, el pipeline solo ha sido pensado para el idioma inglés."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
